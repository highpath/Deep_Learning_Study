{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8-2. Artificial Neural Network.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOBF7rER2SW8HWNtOL3gTgG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kP1OziCEDHw8","colab_type":"text"},"source":["## 2) 인공 신경망(Artificial Neural Network) 훑어보기\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QOiADlzxEXEh","colab_type":"text"},"source":["### 1. 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)\n",": 입력층에서 출력층 방향으로 연산이 전개되는 신경망\n","\n","![Feed-Forward Neural Network](https://wikidocs.net/images/page/24987/mlp_final.PNG)"]},{"cell_type":"markdown","metadata":{"id":"p8zkjJ5-93wC","colab_type":"text"},"source":["### 2. 단층 퍼셉트론(Single-Layer Perceptron)\n","\n","단층 퍼셉트론은 값을 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어집니다. 이때 이 각 단계를 보통층(Layer)라고 부르며, 이 두개의 층을 입력층(input layer)과 출력층(output layer)이라고 합니다.\n","\n","![Single-Layer Perceptron](https://wikidocs.net/images/page/24958/perceptron3_final.PNG)\n","\n","#### 한계\n","\n","단층 퍼셉트론을 이용하면 AND, NAND, OR 게이트를 쉽게 구현할 수 있습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"7EL98530bF0J","colab_type":"text"},"source":["### 2. 전결합층(Fully-connected layer, FC, Dense layer)\n",": 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층을 전결합층, 또는 밀집층이라고 합니다. 케라스에서는 밀집층을 구현할 때 Dense()를 사용합니다."]},{"cell_type":"markdown","metadata":{"id":"gUxgVKxnbekw","colab_type":"text"},"source":["### 3. 활성화 함수(Activation Function)\n","![Activation Function](https://wikidocs.net/images/page/24987/activation_function_final.PNG)\n","\n","#### (1) 활성화 함수의 특징 - 비선형 함수(Nonlinear function)\n","인공 신경망에서의 활성화 함수는 선형 함수가 아닌 비선형 함수여야 합니다. 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해주어야 하는데, 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없습니다.\n","\n","활성화 함수는 $f(x)=Wx$라 가정할 때, \n","\n","#### (2) 계단 함수(Step function)"]},{"cell_type":"code","metadata":{"id":"XyWL4ezudc8J","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDPQ22NydmtT","colab_type":"code","colab":{}},"source":["def step(x):\n","    return np.array(x>0, dtype=np.int)\n","x = np.arange(-5.0, 5.0, 0.1)\n","y = step(x)\n","plt.title('Step Function')\n","plt.plot(x,y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDyz69hBeCAP","colab_type":"text"},"source":["#### (3) 시그모이드 함수(Sigmoid function)와 기울기 소실\n","\n","시그모이드 함수의 출력값이 0또는 1에 가까워지면, 그래프의 기울기가 완만해집니다. 즉, 오차가 작아질 수록 그래프의 기울기가 완만해져 0에 가까운 아주 작은 기울기가 됩니다. 이렇게 되면 앞단에는 기울기가 잘 전달이 되지 않게 되는데 이를 기울기 소실(Vanishing Gradient) 문제라고 합니다.\n","\n","시그모이드 함수를 사용하는 은닉층의 개수가 다수가 될 경우에는 0에 가까운 기울기가 계속 곱해지면 앞단에서는 거의 기울기를 전파받을 수 없게 됩니다. 다시 말해 매개변수 W가 업데이트 되지 않아 학습이 되지를 않습니다."]},{"cell_type":"markdown","metadata":{"id":"5hnXZU_ddclI","colab_type":"text"},"source":["※ 시그모이드는 활성화 함수이고 손실함수는 실제값과 예측값 사이의 오차를 나타낸 함수이지만, 실제값이 0 아니면 1이므로 미분값은 시그모이드, 즉 활성화 함수의 영향을 전적으로 받을 수 밖에 없다."]},{"cell_type":"markdown","metadata":{"id":"wCWv0fati1v8","colab_type":"text"},"source":["#### (4) 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)\n","\n","하이퍼볼릭탄젠트 함수의 경우 시그모이드 함수와는 달리 0을 중심으로 하고 있는데, 이 때문에 시그모이드 함수와 비교하면 반환값의 변화폭이 더 큽니다. 그래서 시그모이드 함수보다는 기울기 소실 증상이 적은 편입니다. 그래서 은닉층에서 시그모이드 함수보다는 많이 사용됩니다."]},{"cell_type":"code","metadata":{"id":"FLUqE7WHi-HM","colab_type":"code","colab":{}},"source":["x = np.arange(-5.0, 5.0, 0.1)\n","y = np.tanh(x)\n","\n","plt.plot(x,y)\n","plt.plot([0,0], [1.0, -1.0], ':')\n","plt.axhline(y=0, color='orange', linestyle='--')\n","plt.title('Tanh Function')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tL55AWopjsDl","colab_type":"text"},"source":["#### (5) 렐루 함수(ReLU)\n","렐루 함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환합니다. 렐루 함수는 특정 양수값에 수렴하지 않으므로 깊은 신경망에서 시그모이드 함수보다 훨씬 더 잘 작동합니다. 뿐만 아니라, 렐루 함수는 시그모이드 함수와 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠릅니다.\n","\n","하지만 여전히 문제점이 존재하는데 입력값이 음수면 기울기도 0이 됩니다. 그리고 이 뉴런은 다시 회생하는 것이 매우 어렵습니다. 이 문제를 죽은 렐루(dying ReLU)라고 합니다.\n","\n","$f(x)=max(0,x)$\n","\n"]},{"cell_type":"code","metadata":{"id":"e-r9I4KCB85q","colab_type":"code","colab":{}},"source":["def relu(x):\n","    return np.maximum(0,x)\n","\n","x = np.arange(-5.0, 5.0, 0.1)\n","y = relu(x)\n","\n","plt.plot(x,y)\n","plt.plot([0,0],[5.0,0.0], ':')\n","plt.title('Relu Function')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NquCYolSCKai","colab_type":"text"},"source":["#### (6) 리키 렐루(Leaky ReLU)\n","죽은 렐루를 보완하기 위해 ReLU의 변형 함수들이 등장하기 시작했습니다. 변형 함수는 여러개가 있지만 여기서는 Leaky ReLU만 소개합니다. Leaky ReLU는 입력값이 음수일 경우에 0이 아니라 0.001과 같은 매우 작은 수를 반환하도록 되어있습니다.\n","\n","$f(x) = max(ax,x)$\n","\n","a는 하이퍼파라미터로 Leaky('새는') 정도를 결정하며 일반적으로는 0.01의 값을 가집니다."]},{"cell_type":"markdown","metadata":{"id":"bWKD-Bx2lqKD","colab_type":"text"},"source":["#### (7) 소프트맥스 함수(Softmax function)\n","\n","은닉층에서 ReLU 함수들을 사용하는 것이 일반적이지만 그렇다고 해서 앞서 배운 시그모이드 함수나 소프트맥스 함수가 사용되지 않는다는 의미는 아닙니다. 분류 문제를 로지스틱 회귀와 소프트맥스 회귀를 출력층에 적용하여 사용합니다.\n","\n","소프트맥스 함수는 시그모이드 함수처럼 출력층의 뉴런에서 주로 사용되는데, 시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류(Binary Classification) 문제에 사용된다면, 세 가지 이상의 (상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classification) 문제에 주로 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"Ju8QoT6mCWPT","colab_type":"text"},"source":["### 4. 행렬의 곱셈을 이용한 순전파(Forward Propagation)\n","\n","인공 신경망에서 입력층에서 출력층 방향으로 연산을 진행하는 과정을 순전파(Forward Propagation)라고 합니다. 다르게 말하면 주어진 입력으로부터 예측값을 계산하는 과정을 순전파라고 합니다. 앞서 머신 러닝 챕터에서 배웠던 벡터와 행렬 연산을 인공 신경망에 적용하려고 하면 벡터와 행렬 연산이 순전파 과정에서 층(layer)마다 적용이 됩니다.\n","\n","#### 1) layer1 의 행렬 크기 추정하기\n","우선 각 층을 기준으로 입력과 출력의 개수를 정리하면 다음과 같습니다.\n","\n","편의상 입력층을 layer 0, 은닉층 1을 layer1, 은닉층 2를 layer2, 출력층을 layer3라고 해봅시다.\n","\n","layer0: 4개의 입력과 8개의 출력  \n","layer1: 8개의 입력과 8개의 출력  \n","layer2: 8개의 입력과 3개의 출력  \n","layer3: 3개의 입력과 3개의 출력\n","\n","위 정보를 가지고 층마다 생기는 가중치와 편향 행렬의 크기를 추정해봅시다.\n","배치 크기는 1로 합니다. 이 경우 layer 1에서 처음 입력으로 들어오는 입력 행렬 X의 크기는 1x4로 행벡터에 해당됩니다.\n","\n","$X_{1 \\times 4} \\times W_{4 \\times 8} + B_{1 \\times 8} = Y_{1 \\times 8}$\n","\n","#### 2) layer 2와 layer 3의 행렬 크기 추정하기\n","layer 2:\n","$X_{1 \\times 8} \\times W_{8 \\times 8} + B_{1 \\times 8} = Y_{1 \\times 8}$\n","\n","layer 3:\n","$X_{1 \\times 8} \\times W_{8 \\times 3} + B_{1 \\times 3} = Y_{1 \\times 3}$\n","\n","이와 같이 순전파를 진행하고 예측값을 구하고 나서 이 다음에 인공 신경망이 해야할 일은 예측값과 실제값으로부터 오차를 계산하고, 오차로부터 가중치와 편향을 업데이트하는 일입니다. 즉, 인공 신경망의 학습 단계에 해당됩니다. 이때, 인공 신경망은 순전파와는 반대 방향으로 연산을 진행하며 가중치를 업데이트하는데, 이 과정을 역전파(Back Propagation)라고 합니다."]},{"cell_type":"markdown","metadata":{"id":"_CwGztbQ8ETP","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"uQwDlt_tCaj2","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","model = Sequential()\n","model.add(Dense(8, input_dim=4, init='uniform', activation='relu'))\n","# 입력층(4)과 다음 은닉층(8), 그리고 은닉층의 활성화 함수는 relu\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(3, activation='softmax'))"],"execution_count":null,"outputs":[]}]}