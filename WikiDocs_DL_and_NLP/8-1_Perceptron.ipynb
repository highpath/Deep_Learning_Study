{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8-1. Perceptron.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPzPiM93X6bx0MDdd28+UmX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kP1OziCEDHw8","colab_type":"text"},"source":["## 1) 퍼셉트론(Perceptron)\n","인공 신경망은 수많은 머신 러닝 방법 중 하나입니다. 인공 신경망을 복잡하게 쌓아 올린 딥 러닝을 이해하기 위해서 초기의 인공 신경망인 퍼셉트론(Perceptron)에 대해서 알아봅니다.\n"]},{"cell_type":"markdown","metadata":{"id":"QOiADlzxEXEh","colab_type":"text"},"source":["### 1. 퍼셉트론(Perceptron)\n","\n","#### 뉴런과 퍼셉트론\n","퍼셉트론(Perceptron)은 초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘입니다. 퍼셉트론은 실제 뇌를 구성하는 신경 세포 뉴런의 동작과 유사합니다.\n","\n","![Neuron](https://wikidocs.net/images/page/24958/%EB%89%B4%EB%9F%B0.PNG)\n","![Perceptron](https://wikidocs.net/images/page/24958/perceptrin1_final.PNG)\n","\n","$x$는 입력값, $W$는 가중치(Weight), $y$는 출력값, 그림 안의 원은 인공 뉴런에 해당합니다. 각각의 입력값에는 각각의 가중치가 존재하고, 가중치의 값이 클수록 해당 입력 값이 중요하다는 것을 의미합니다."]},{"cell_type":"markdown","metadata":{"id":"VrOaZ-GcEXgv","colab_type":"text"},"source":["#### 계단 함수\n","각 입력값이 가중치와 곱해져서 인공 뉴런에 보내집니다. 그 곱들의 전체 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로 1을 출력하고, 그렇지 않을 경우 0을 출력합니다. 이러한 함수를 계단 함수(Step function)라고 합니다.\n","\n","![Step Function](https://wikidocs.net/images/page/24987/step_function.PNG)\n","\n","$ if \\ \\sum_i^n W_ix_i+b \\geq \\theta $ -> $ y=1 $\n","\n","$ if \\ \\sum_i^n W_ix_i+b < \\theta $ -> $ y=1 $\n","\n","위의 식에서 임계치를 좌변으로 넘기고 $b$(bias)로 표현할 수도 있습니다. 편향 $b$ 또한 퍼셉트론의 입력으로 사용됩니다. 보통 그림으로 표현할 때는 입력값이 1로 고정되고 편향 b가 곱해지는 변수로 표현됩니다.\n","\n","![Perceptron_bias](https://wikidocs.net/images/page/24958/perceptron2_final.PNG)\n","\n","$if \\ \\sum_i^n W_ix_i + b \\leq 0$ -> $y=1$\n","\n","$if \\ \\sum_i^n W_ix_i + b < 0$ -> $y=0$\n"]},{"cell_type":"markdown","metadata":{"id":"p8zkjJ5-93wC","colab_type":"text"},"source":["### 2. 단층 퍼셉트론(Single-Layer Perceptron)\n","\n","단층 퍼셉트론은 값을 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어집니다. 이때 이 각 단계를 보통층(Layer)라고 부르며, 이 두개의 층을 입력층(input layer)과 출력층(output layer)이라고 합니다.\n","\n","![Single-Layer Perceptron](https://wikidocs.net/images/page/24958/perceptron3_final.PNG)\n","\n","#### 한계\n","\n","단층 퍼셉트론을 이용하면 AND, NAND, OR 게이트를 쉽게 구현할 수 있습니다.\n"]},{"cell_type":"code","metadata":{"id":"e-r9I4KCB85q","colab_type":"code","colab":{}},"source":["def AND_gate(x1, x2):\n","    w1=0.5\n","    w2=0.5\n","    b=-0.7\n","    result = x1*w1 + x2*w2 + b\n","    if result <=0:\n","        return 0\n","    else:\n","        return 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NquCYolSCKai","colab_type":"code","colab":{}},"source":["AND_gate(0,0), AND_gate(0,1), AND_gate(1,0), AND_gate(1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ju8QoT6mCWPT","colab_type":"code","colab":{}},"source":["def NAND_gate(x1, x2):\n","    w1=-0.5\n","    w2=-0.5\n","    b=0.7\n","    result = x1*w1 + x2*w2 + b\n","    if result <= 0:\n","        return 0\n","    else:\n","        return 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQwDlt_tCaj2","colab_type":"code","colab":{}},"source":["NAND_gate(0,0), NAND_gate(0,1), NAND_gate(1,0), NAND_gate(1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RheCYJhVCgUr","colab_type":"text"},"source":["### 3. 다층 퍼셉트론(MultiLayer Perceptron, MLP)\n","![MultiLayer Perceptron](https://wikidocs.net/images/page/24958/perceptron_4image.jpg)\n","\n","은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 합니다. "]}]}