{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8-3-4. BackPropagation.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNEybULMcbB/S0HGYc8mwTf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bAt5_DT_awMY","colab_type":"text"},"source":["## 3-4) 역전파(BackPropagation) 이해하기\n","어떻게 역전파 과정에서 경사 하강법을 사용하여 가중치를 업데이트하는지 직접 계산을 통해 이해해봅시다."]},{"cell_type":"markdown","metadata":{"id":"7eHMp4q2bcG8","colab_type":"text"},"source":["### 1. 인공 신경망의 이해(Neural Network Overview)\n","\n","역전파의 이해를 위해서 여기서 사용할 인공 신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가집니다. 또한 해당 인공 신경망은 두 개의 입력과, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런을 사용합니다. 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용합니다.\n","\n","![예제](https://wikidocs.net/images/page/37406/nn1_final.PNG)\n","\n","여기서 변수 $z$는 이전 층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미합니다. 이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태입니다. 즉, 활성화 함수의 입력을 의미합니다.\n","\n","변수 $h$ 또는 $o$는 z가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값을 의미합니다."]},{"cell_type":"markdown","metadata":{"id":"yQskvMURwbgk","colab_type":"text"},"source":["### 3. 역전파 1단계(BackPropagation Step1)\n","역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해갑니다. 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 업데이트 하는 단계를 역전파 1단계, 그리고 N층과 N층의 이전층 사이의 가중치를 업데이트 하는 단계를 역전파 2단계라고 해봅시다.\n","\n"]}]}