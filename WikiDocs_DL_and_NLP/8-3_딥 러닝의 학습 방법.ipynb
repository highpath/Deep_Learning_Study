{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8-3. 딥 러닝의 학습 방법.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPakCiQ6qNnzS8XTARokE/m"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"11oO8MiDJQUm","colab_type":"text"},"source":["## 3) 딥 러닝의 학습 방법"]},{"cell_type":"markdown","metadata":{"id":"xUgUoXEFJpTd","colab_type":"text"},"source":["### 1. 순전파(Forward Propagation)\n","![Forward Propagation](https://wikidocs.net/images/page/36033/%EC%88%9C%EC%A0%84%ED%8C%8C.PNG)"]},{"cell_type":"markdown","metadata":{"id":"0xqqg5PTK9Ns","colab_type":"text"},"source":["### 2. 손실 함수(Loss function)\n","![Loss function](https://wikidocs.net/images/page/36033/%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98.PNG)\n","\n","#### 1) MSE(Mean Squared Error, MSE)\n","오차 제곱 평균. 연속형 변수를 예측할 때 사용됩니다.\n","\n","#### 2) 크로스 엔트로피(Cross-Entropy)\n","낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 큽니다. 이진 분류(Binary Classification)의 경우 binary_crossentropy를 사용하며 다중 클래스 분류(Multi-Class Classification)일 경우 categorical_crossentropy를 사용합니다."]},{"cell_type":"markdown","metadata":{"id":"OdsFJyESLilC","colab_type":"text"},"source":["### 3. 옵티마이저(Optimizer)\n","손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라집니다. 여기서 배치(Batch)라는 개념에 대한 이해가 필요합니다. 배치는 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양을 말합니다. 전체 데이터를 가지고 매개 변수의 값을 조정할 수도 있고, 정해준 양의 데이터만 가지고도 매개 변수의 값을 조정할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"70rfPKhnMhOD","colab_type":"text"},"source":["#### 1) 배치 경사 하강법(Batch Gradient Descent)\n","가장 기본적인 경사 하강법입니다. 옵티마이저 중 하나로 오차(loss)를 구할 때 전체 데이터를 고려합니다. 머신 러닝에서는 1번의 훈련 횟수를 1에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행합니다. 배치 경사 하강법은 전체 데이터를 고려해서 학습하므로 에포크당 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있으나, 글로벌 미니멈을 찾을 수 있다는 장점이 있습니다.\n","\n","`model.fit(X_train, y_train, batch_size=len(trainX))`"]},{"cell_type":"markdown","metadata":{"id":"U9OIOgtLNDBV","colab_type":"text"},"source":["#### 2) 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n","매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법입니다. 더 적은 데이터를 사용하므로 더 빠르게 계산할 수 있습니다. 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수도 있지만 속도만큼은 배치 경사 하강법보다 빠르다는 장점이 있습니다.\n","\n","![SGD](https://wikidocs.net/images/page/24987/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95SGD.PNG)\n","\n","`model.fit(X_train, y_train, batch_size=1)`"]},{"cell_type":"markdown","metadata":{"id":"dVu4LvU1OvBQ","colab_type":"text"},"source":["### 3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)\n","\n","전체 데이터도 아니고, 1개의 데이터도 아니고 정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법을 미니 배치 경사 하강법이라고 합니다. 미니 배치 경사 하강법은 전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이라는 장점이 있습니다. 실제로 가장 많이 사용되는 경사 하강법입니다.\n","\n","`model.fit(X_train, y_train, batch_size=32)`"]},{"cell_type":"markdown","metadata":{"id":"EVWMtTpKL7zd","colab_type":"text"},"source":["### 4) 모멘텀(Momentum)\n","모멘텀(Momentum)은 관성이라는 물리학의 법칙을 응용한 방법입니다. 모멘텀 SGD는 경사 하강법에 관성을 더해줍니다. 모멘텀은 SGD에서 접선의 기울기에 한 시점(step) 전의 기울기값을 일정한 비율만큼 반영합니다. 이렇게 하면 마치 언덕에서 공이 내려올 때, 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과를 줄 수 있습니다.\n","\n","![Momentum](https://wikidocs.net/images/page/24987/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG)\n","\n","`keras.optimizers.SGD(lr = 0.01, momentum=0.9)`"]},{"cell_type":"markdown","metadata":{"id":"kcPR6-KUMs93","colab_type":"text"},"source":["### 5) 아다그라드(Adagrad)\n","\n","매개변수들은 각자 의미하는 바가 다른데, 모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적입니다. 아다그라드는 각 매개변수에 서로 다른 학습률을 적용시킵니다. 이 때, 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정시킵니다.\n","\n","`keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)"]},{"cell_type":"markdown","metadata":{"id":"Ejed0LeNM57d","colab_type":"text"},"source":["### 6) 알엠에스프롭(RMSprop)\n","\n","아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서는 학습률이 지나치게 떨어진다는 단점이 있는데 이를 다른 수식으로 대체하여 이러한 단점을 개선하였습니다.\n","\n","`keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)`"]},{"cell_type":"markdown","metadata":{"id":"vHcBRmkRPGOl","colab_type":"text"},"source":["### 7) 아담(Adam)\n","\n","아담은 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법입니다.\n","\n","`keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)`\n","\n","[케라스의 옵티마이저 사용법](https://keras.io/optimizers/)"]},{"cell_type":"markdown","metadata":{"id":"61j-fRxrPkHw","colab_type":"text"},"source":["## 4. 역전파(BackPropagation)"]},{"cell_type":"markdown","metadata":{"id":"_3Wzx0VgPpfG","colab_type":"text"},"source":["## 5. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)\n","\n"]}]}